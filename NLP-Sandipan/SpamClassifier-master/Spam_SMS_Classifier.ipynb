{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d6hhXkWNQre"
      },
      "outputs": [],
      "source": [
        "# Importing essential libraries\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# Loading the dataset\n",
        "df = pd.read_csv('Spam SMS Collection', sep='\\t', names=['label', 'message'])\n",
        "\n",
        "# Importing essential libraries for performing Natural Language Processing on 'SMS Spam Collection' dataset\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Cleaning the messages\n",
        "corpus = []\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for i in range(0,df.shape[0]):\n",
        "\n",
        "  # Cleaning special character from the message\n",
        "  message = re.sub(pattern='[^a-zA-Z]', repl=' ', string=df.message[i])\n",
        "\n",
        "  # Converting the entire message into lower case\n",
        "  message = message.lower()\n",
        "\n",
        "  # Tokenizing the review by words\n",
        "  words = message.split()\n",
        "\n",
        "  # Removing the stop words\n",
        "  words = [word for word in words if word not in set(stopwords.words('english'))]\n",
        "\n",
        "  # Stemming the words\n",
        "  words = [ps.stem(word) for word in words]\n",
        "\n",
        "  # Joining the stemmed words\n",
        "  message = ' '.join(words)\n",
        "\n",
        "  # Building a corpus of messages\n",
        "  corpus.append(message)\n",
        "\n",
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=2500)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "# Extracting dependent variable from the dataset\n",
        "y = pd.get_dummies(df['label'])\n",
        "y = y.iloc[:, 1].values\n",
        "\n",
        "# Creating a pickle file for the CountVectorizer\n",
        "pickle.dump(cv, open('cv-transform.pkl', 'wb'))\n",
        "\n",
        "# Model Building\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB(alpha=0.3)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Creating a pickle file for the Multinomial Naive Bayes model\n",
        "filename = 'spam-sms-mnb-model.pkl'\n",
        "pickle.dump(classifier, open(filename, 'wb'))"
      ]
    }
  ]
}